{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIXHRSSbEFg4",
        "outputId": "d2490d87-72cf-40de-dba0-7471ae272445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.14.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "# !/usr/bin/env python3\n",
        "!pip install torch>=2.0.0\n",
        "!pip install torchvision>=0.15.0\n",
        "!pip install numpy>=1.24.0\n",
        "!pip install Pillow>=9.0.0\n",
        "!pip install opencv-python>=4.5.0\n",
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ol0XCDWrEIBf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models.segmentation as seg_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bu9uoqIsEIEh"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class VideoSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects root_dir with structure:\n",
        "      root_dir/\n",
        "        video_01/\n",
        "          frames_original/\n",
        "            000001.png\n",
        "            ...\n",
        "          segmentation/\n",
        "            000001.png\n",
        "            ...\n",
        "        video_02/...\n",
        "    images/frame_original are rgb images\n",
        "    Masks/segmentations are grayscale with values 0..9 (integers, 0=background).\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, img_size=(256,256), augment=False):\n",
        "        self.samples = []\n",
        "        self.img_size = tuple(img_size)\n",
        "        self.augment = augment\n",
        "\n",
        "        if not os.path.isdir(root_dir):\n",
        "            raise ValueError(f\"{root_dir} is not a directory\")\n",
        "\n",
        "        videos = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        for video in videos:\n",
        "            frame_dir = os.path.join(root_dir, video, \"frames_original\")\n",
        "            mask_dir = os.path.join(root_dir, video, \"segmentation\")\n",
        "            if not os.path.isdir(frame_dir) or not os.path.isdir(mask_dir):\n",
        "                continue\n",
        "            frames = sorted(glob.glob(os.path.join(frame_dir, \"*.png\")))\n",
        "            for f in frames:\n",
        "                filename = os.path.basename(f)\n",
        "                mask_path = os.path.join(mask_dir, filename)\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.samples.append((f, mask_path))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(f\"No samples found in {root_dir}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _sync_transform(self, image, mask):\n",
        "        image = image.resize(self.img_size, resample=Image.BILINEAR)\n",
        "        mask = mask.resize(self.img_size, resample=Image.NEAREST)\n",
        "\n",
        "        # Random horizontal flip\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = TF.to_tensor(image)  # float [0,1] CxHxW\n",
        "        image = TF.normalize(image, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "\n",
        "        mask = np.array(mask, dtype=np.int64)\n",
        "        mask = torch.from_numpy(mask)\n",
        "        return image, mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        m = Image.open(mask_path).convert(\"L\")\n",
        "        img, m = self._sync_transform(img, m)\n",
        "        return img, m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-OLEXFqwEIHX"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Metrics\n",
        "# -------------------------\n",
        "def fast_confusion_matrix(preds, labels, num_classes):\n",
        "    \"\"\"\n",
        "    preds: HxW or (N,H,W) LongTensor predicted class ids\n",
        "    labels: same shape ground-truth\n",
        "    returns: (num_classes, num_classes) confusion matrix where\n",
        "      conf[i,j] = count of pixels where true=i and pred=j\n",
        "    \"\"\"\n",
        "    preds_np = preds.cpu().numpy().ravel()\n",
        "    labels_np = labels.cpu().numpy().ravel()\n",
        "    mask = (labels_np >= 0) & (labels_np < num_classes)\n",
        "    hist = np.bincount(\n",
        "        num_classes * labels_np[mask].astype(int) + preds_np[mask].astype(int),\n",
        "        minlength=num_classes**2\n",
        "    ).reshape(num_classes, num_classes)\n",
        "    return hist\n",
        "\n",
        "def compute_metrics_from_confusion(conf, ignore_index=None):\n",
        "    \"\"\"\n",
        "    conf: confusion matrix num_classes x num_classes\n",
        "    Returns dict with per-class IoU, mean IoU, pixel accuracy\n",
        "    \"\"\"\n",
        "    tp = np.diag(conf).astype(np.float64)\n",
        "    pos_gt = conf.sum(axis=1).astype(np.float64)\n",
        "    pos_pred = conf.sum(axis=0).astype(np.float64)\n",
        "    union = pos_gt + pos_pred - tp\n",
        "\n",
        "    eps = 1e-12\n",
        "    iou = tp / (union + eps)\n",
        "    # If a class has zero gt pixels, set IoU to nan\n",
        "    iou[pos_gt == 0] = np.nan\n",
        "\n",
        "    mean_iou = np.nanmean(iou)\n",
        "    pixel_acc = tp.sum() / (conf.sum() + eps)\n",
        "\n",
        "    per_class = {f\"class_{c}\": float(iou[c]) if not np.isnan(iou[c]) else None\n",
        "                 for c in range(len(iou))}\n",
        "    metrics = {\"per_class_iou\": per_class, \"mean_iou\": float(mean_iou), \"pixel_acc\": float(pixel_acc)}\n",
        "    return metrics\n",
        "\n",
        "# -------------------------\n",
        "# Focal Loss\n",
        "# -------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, ignore_index=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # Only pass ignore_index if it's not None othrwise was giving error\n",
        "        if self.ignore_index is not None:\n",
        "            ce_loss = nn.CrossEntropyLoss(weight=self.alpha, ignore_index=self.ignore_index, reduction='none')(logits, targets)\n",
        "        else:\n",
        "            ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none')(logits, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        return focal_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nL2ZCsEaEIJ_"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Training / Validation loops\n",
        "# -------------------------\n",
        "def train_one_epoch(model, dataloader, optimizer, device, criterion, num_classes, epoch, log_every=20):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    conf = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for i, (images, masks) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)['out']  # (N, C, H, W)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)  # (N,H,W)\n",
        "        conf += fast_confusion_matrix(preds, masks, num_classes)\n",
        "\n",
        "        if (i + 1) % log_every == 0:\n",
        "            print(f\"  [Epoch {epoch}] Iter {i+1}/{len(dataloader)}  loss={loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / (len(dataloader.dataset))\n",
        "    metrics = compute_metrics_from_confusion(conf, ignore_index=None)\n",
        "    metrics['loss'] = epoch_loss\n",
        "    return metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, dataloader, device, criterion, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    conf = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs, masks)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        conf += fast_confusion_matrix(preds, masks, num_classes)\n",
        "\n",
        "    epoch_loss = running_loss / (len(dataloader.dataset))\n",
        "    metrics = compute_metrics_from_confusion(conf, ignore_index=None)\n",
        "    metrics['loss'] = epoch_loss\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BWTP82heEIMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133a9145-bfa3-4f63-8844-9fc7464f88ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Arguments\n",
        "# -------------------------\n",
        "\n",
        "train_dir='/content/drive/MyDrive/endovis256/train/'\n",
        "val_dir='/content/drive/MyDrive/endovis256/test/'\n",
        "out_dir='/content/drive/MyDrive/endovis256/focal_checkpoints/'\n",
        "epochs=20\n",
        "batch_size=16\n",
        "img_size=256\n",
        "lr=1e-4\n",
        "num_workers=32\n",
        "num_classes=10\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "esN3X4BbEIPA"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "\n",
        "def main():\n",
        "\n",
        "    print(out_dir)\n",
        "    #repeated as sometime collab notebook was behaving wierdly\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(device)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    device = torch.device(device)\n",
        "\n",
        "    print(\"Preparing datasets...\")\n",
        "    train_dataset = VideoSegmentationDataset(train_dir, img_size=(img_size, img_size), augment=True)\n",
        "    val_dataset = VideoSegmentationDataset(val_dir, img_size=(img_size, img_size), augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    print(\"Creating model...\")\n",
        "    model = seg_models.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
        "    # for using the pretrained model, uncomment the lines below instead\n",
        "    # TODO: update path below\n",
        "    # model = seg_models.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
        "    # ckpt = torch.load('/content/drive/MyDrive/endovis256/focal_checkpoints/best_model.pth', map_location=device)\n",
        "    # model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    ce_criterion = nn.CrossEntropyLoss()\n",
        "    focal_criterion = FocalLoss(gamma=2.0)\n",
        "\n",
        "    def combined_loss(logits, targets):\n",
        "        return ce_criterion(logits, targets) + focal_criterion(logits, targets)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "    best_miou = -1.0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
        "        train_metrics = train_one_epoch(model, train_loader, optimizer, device, combined_loss, num_classes, epoch)\n",
        "        print(f\"Train loss {train_metrics['loss']:.4f}  pix_acc {train_metrics['pixel_acc']:.4f}  mean_iou {train_metrics['mean_iou']:.4f}\")\n",
        "\n",
        "        val_metrics = validate(model, val_loader, device, combined_loss, num_classes)\n",
        "        print(f\"Val   loss {val_metrics['loss']:.4f}  pix_acc {val_metrics['pixel_acc']:.4f}  mean_iou {val_metrics['mean_iou']:.4f}\")\n",
        "        for k, v in val_metrics['per_class_iou'].items():\n",
        "            print(f\"  {k}: {v}\")\n",
        "\n",
        "        scheduler.step(val_metrics['mean_iou'])\n",
        "\n",
        "        # Save checkpoint\n",
        "        # ckpt_path = os.path.join(out_dir, f\"epoch_{epoch:03d}.pth\")\n",
        "        # torch.save({\n",
        "        #     'epoch': epoch,\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #     'miou': val_metrics['mean_iou']\n",
        "        # }, ckpt_path)\n",
        "\n",
        "        if val_metrics['mean_iou'] > best_miou:\n",
        "            best_miou = val_metrics['mean_iou']\n",
        "            best_path = os.path.join(out_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'miou': val_metrics['mean_iou']\n",
        "            }, best_path)\n",
        "            print(f\"  -> New best model saved to {best_path} (mIoU={best_miou:.4f})\")\n",
        "\n",
        "    print(\"Training finished. Best val mIoU:\", best_miou)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFAvirb2EIRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b87ca0-15da-4eff-e5ad-9b92333da06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/endovis256/focal_checkpoints/\n",
            "cuda\n",
            "Preparing datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 134MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Epoch 1/20 ===\n",
            "  [Epoch 1] Iter 20/816  loss=0.1293\n",
            "  [Epoch 1] Iter 40/816  loss=0.1473\n",
            "  [Epoch 1] Iter 60/816  loss=0.1099\n",
            "  [Epoch 1] Iter 80/816  loss=0.1636\n",
            "  [Epoch 1] Iter 100/816  loss=0.1565\n",
            "  [Epoch 1] Iter 120/816  loss=0.1383\n",
            "  [Epoch 1] Iter 140/816  loss=0.2010\n",
            "  [Epoch 1] Iter 160/816  loss=0.1351\n",
            "  [Epoch 1] Iter 180/816  loss=0.1402\n",
            "  [Epoch 1] Iter 200/816  loss=0.1317\n",
            "  [Epoch 1] Iter 220/816  loss=0.2174\n",
            "  [Epoch 1] Iter 240/816  loss=0.1551\n",
            "  [Epoch 1] Iter 260/816  loss=0.1323\n",
            "  [Epoch 1] Iter 280/816  loss=0.1416\n",
            "  [Epoch 1] Iter 300/816  loss=0.1475\n",
            "  [Epoch 1] Iter 320/816  loss=0.1691\n",
            "  [Epoch 1] Iter 340/816  loss=0.1328\n",
            "  [Epoch 1] Iter 360/816  loss=0.1582\n",
            "  [Epoch 1] Iter 380/816  loss=0.1466\n",
            "  [Epoch 1] Iter 400/816  loss=0.1277\n",
            "  [Epoch 1] Iter 420/816  loss=0.1027\n",
            "  [Epoch 1] Iter 440/816  loss=0.1614\n",
            "  [Epoch 1] Iter 460/816  loss=0.1359\n",
            "  [Epoch 1] Iter 480/816  loss=0.1745\n",
            "  [Epoch 1] Iter 500/816  loss=0.1324\n",
            "  [Epoch 1] Iter 520/816  loss=0.1264\n",
            "  [Epoch 1] Iter 540/816  loss=0.1163\n",
            "  [Epoch 1] Iter 560/816  loss=0.0994\n",
            "  [Epoch 1] Iter 580/816  loss=0.1106\n",
            "  [Epoch 1] Iter 600/816  loss=0.1357\n",
            "  [Epoch 1] Iter 620/816  loss=0.1356\n",
            "  [Epoch 1] Iter 640/816  loss=0.1394\n",
            "  [Epoch 1] Iter 660/816  loss=0.1325\n",
            "  [Epoch 1] Iter 680/816  loss=0.1385\n",
            "  [Epoch 1] Iter 700/816  loss=0.1063\n",
            "  [Epoch 1] Iter 720/816  loss=0.1069\n",
            "  [Epoch 1] Iter 740/816  loss=0.1313\n",
            "  [Epoch 1] Iter 760/816  loss=0.1571\n",
            "  [Epoch 1] Iter 780/816  loss=0.1304\n",
            "  [Epoch 1] Iter 800/816  loss=0.1083\n",
            "Train loss 0.1402  pix_acc 0.9658  mean_iou 0.6655\n",
            "Val   loss 0.2157  pix_acc 0.9546  mean_iou 0.6192\n",
            "  class_0: 0.9570175823598688\n",
            "  class_1: 0.6660135403567827\n",
            "  class_2: 0.7144353378780813\n",
            "  class_3: 0.8968065820875795\n",
            "  class_4: 0.46462586797518746\n",
            "  class_5: 0.22531406068853077\n",
            "  class_6: 0.636043880814813\n",
            "  class_7: 0.7388886700283643\n",
            "  class_8: 0.13808767479918685\n",
            "  class_9: 0.7544828429374847\n",
            "  -> New best model saved to /content/drive/MyDrive/endovis256/focal_checkpoints/best_model.pth (mIoU=0.6192)\n",
            "\n",
            "=== Epoch 2/20 ===\n",
            "  [Epoch 2] Iter 20/816  loss=0.1287\n",
            "  [Epoch 2] Iter 40/816  loss=0.1407\n",
            "  [Epoch 2] Iter 60/816  loss=0.1108\n",
            "  [Epoch 2] Iter 80/816  loss=0.1353\n",
            "  [Epoch 2] Iter 100/816  loss=0.1742\n",
            "  [Epoch 2] Iter 120/816  loss=0.1452\n",
            "  [Epoch 2] Iter 140/816  loss=0.1476\n",
            "  [Epoch 2] Iter 160/816  loss=0.1786\n",
            "  [Epoch 2] Iter 180/816  loss=0.1095\n",
            "  [Epoch 2] Iter 200/816  loss=0.0966\n",
            "  [Epoch 2] Iter 220/816  loss=0.1349\n",
            "  [Epoch 2] Iter 240/816  loss=0.1534\n",
            "  [Epoch 2] Iter 260/816  loss=0.1218\n",
            "  [Epoch 2] Iter 280/816  loss=0.1181\n",
            "  [Epoch 2] Iter 300/816  loss=0.1303\n",
            "  [Epoch 2] Iter 320/816  loss=0.1319\n",
            "  [Epoch 2] Iter 340/816  loss=0.1215\n",
            "  [Epoch 2] Iter 360/816  loss=0.1214\n",
            "  [Epoch 2] Iter 380/816  loss=0.1099\n",
            "  [Epoch 2] Iter 400/816  loss=0.1003\n",
            "  [Epoch 2] Iter 420/816  loss=0.1042\n",
            "  [Epoch 2] Iter 440/816  loss=0.1240\n",
            "  [Epoch 2] Iter 460/816  loss=0.1076\n",
            "  [Epoch 2] Iter 480/816  loss=0.1070\n",
            "  [Epoch 2] Iter 500/816  loss=0.1121\n",
            "  [Epoch 2] Iter 520/816  loss=0.0975\n",
            "  [Epoch 2] Iter 540/816  loss=0.1540\n",
            "  [Epoch 2] Iter 560/816  loss=0.1689\n",
            "  [Epoch 2] Iter 580/816  loss=0.1285\n",
            "  [Epoch 2] Iter 600/816  loss=0.1370\n",
            "  [Epoch 2] Iter 620/816  loss=0.1104\n",
            "  [Epoch 2] Iter 640/816  loss=0.1040\n",
            "  [Epoch 2] Iter 660/816  loss=0.1141\n",
            "  [Epoch 2] Iter 680/816  loss=0.1286\n",
            "  [Epoch 2] Iter 700/816  loss=0.1125\n",
            "  [Epoch 2] Iter 720/816  loss=0.1347\n",
            "  [Epoch 2] Iter 740/816  loss=0.1305\n",
            "  [Epoch 2] Iter 760/816  loss=0.1293\n",
            "  [Epoch 2] Iter 780/816  loss=0.1444\n",
            "  [Epoch 2] Iter 800/816  loss=0.1070\n",
            "Train loss 0.1294  pix_acc 0.9675  mean_iou 0.6844\n",
            "Val   loss 0.2109  pix_acc 0.9553  mean_iou 0.6153\n",
            "  class_0: 0.9579407032705479\n",
            "  class_1: 0.6814170024775764\n",
            "  class_2: 0.7345858400389753\n",
            "  class_3: 0.8983164797874885\n",
            "  class_4: 0.4568584905272196\n",
            "  class_5: 0.24711013962287118\n",
            "  class_6: 0.6036619858500368\n",
            "  class_7: 0.6838773862695049\n",
            "  class_8: 0.12926245815065496\n",
            "  class_9: 0.7595841664689734\n",
            "\n",
            "=== Epoch 3/20 ===\n",
            "  [Epoch 3] Iter 20/816  loss=0.1083\n",
            "  [Epoch 3] Iter 40/816  loss=0.1110\n",
            "  [Epoch 3] Iter 60/816  loss=0.1193\n",
            "  [Epoch 3] Iter 80/816  loss=0.1250\n",
            "  [Epoch 3] Iter 100/816  loss=0.1073\n",
            "  [Epoch 3] Iter 120/816  loss=0.1268\n",
            "  [Epoch 3] Iter 140/816  loss=0.1073\n",
            "  [Epoch 3] Iter 160/816  loss=0.1284\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_rav2-DJlhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WMmUdLaEIUk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}