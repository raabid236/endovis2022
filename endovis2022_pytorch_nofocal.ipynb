{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !/usr/bin/env python3\n",
        "!pip install torch>=2.0.0\n",
        "!pip install torchvision>=0.15.0\n",
        "!pip install numpy>=1.24.0\n",
        "!pip install Pillow>=9.0.0\n",
        "!pip install opencv-python>=4.5.0\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_MjHGqlQMi",
        "outputId": "ba298617-8bb4-4bb6-bec0-633d3260c630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.14.1)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models.segmentation as seg_models\n"
      ],
      "metadata": {
        "id": "ijMo-w2zvDWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class VideoSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects root_dir with structure:\n",
        "      root_dir/\n",
        "        video_01/\n",
        "          frames_original/\n",
        "            000001.png\n",
        "            ...\n",
        "          segmentation/\n",
        "            000001.png\n",
        "            ...\n",
        "        video_02/...\n",
        "    images/frame_original are rgb images\n",
        "    Masks/segmentations are grayscale with values 0..9 (integers, 0=background).\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, img_size=(256,256), augment=False):\n",
        "        self.samples = []\n",
        "        self.img_size = tuple(img_size)\n",
        "        self.augment = augment\n",
        "\n",
        "        if not os.path.isdir(root_dir):\n",
        "            raise ValueError(f\"{root_dir} is not a directory\")\n",
        "\n",
        "        videos = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        for video in videos:\n",
        "            frame_dir = os.path.join(root_dir, video, \"frames_original\")\n",
        "            mask_dir = os.path.join(root_dir, video, \"segmentation\")\n",
        "            if not os.path.isdir(frame_dir) or not os.path.isdir(mask_dir):\n",
        "                continue\n",
        "            frames = sorted(glob.glob(os.path.join(frame_dir, \"*.png\")))\n",
        "            for f in frames:\n",
        "                filename = os.path.basename(f)\n",
        "                mask_path = os.path.join(mask_dir, filename)\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.samples.append((f, mask_path))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(f\"No samples found in {root_dir}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _sync_transform(self, image, mask):\n",
        "        image = image.resize(self.img_size, resample=Image.BILINEAR)\n",
        "        mask = mask.resize(self.img_size, resample=Image.NEAREST)\n",
        "\n",
        "        # Random horizontal flip\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = TF.to_tensor(image)  # CxHxW\n",
        "        image = TF.normalize(image, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "\n",
        "        mask = np.array(mask, dtype=np.int64)\n",
        "        mask = torch.from_numpy(mask)\n",
        "        return image, mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        m = Image.open(mask_path).convert(\"L\")\n",
        "        img, m = self._sync_transform(img, m)\n",
        "        return img, m\n"
      ],
      "metadata": {
        "id": "HTuMtvnSn2HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# Metrics\n",
        "# -------------------------\n",
        "def fast_confusion_matrix(preds, labels, num_classes):\n",
        "    \"\"\"\n",
        "    preds: HxW or (N,H,W) LongTensor predicted class ids\n",
        "    labels: same shape ground-truth\n",
        "    returns: (num_classes, num_classes) confusion matrix where\n",
        "      conf[i,j] = count of pixels where true=i and pred=j\n",
        "    \"\"\"\n",
        "    preds_np = preds.cpu().numpy().ravel()\n",
        "    labels_np = labels.cpu().numpy().ravel()\n",
        "    mask = (labels_np >= 0) & (labels_np < num_classes)\n",
        "    hist = np.bincount(\n",
        "        num_classes * labels_np[mask].astype(int) + preds_np[mask].astype(int),\n",
        "        minlength=num_classes**2\n",
        "    ).reshape(num_classes, num_classes)\n",
        "    return hist\n",
        "\n",
        "def compute_metrics_from_confusion(conf, ignore_index=None):\n",
        "    \"\"\"\n",
        "    conf: confusion matrix num_classes x num_classes\n",
        "    Returns dict with per-class IoU, mean IoU, pixel accuracy\n",
        "    \"\"\"\n",
        "    tp = np.diag(conf).astype(np.float64)\n",
        "    pos_gt = conf.sum(axis=1).astype(np.float64)  # ground truth per class\n",
        "    pos_pred = conf.sum(axis=0).astype(np.float64)  # predicted per class\n",
        "    union = pos_gt + pos_pred - tp\n",
        "\n",
        "    eps = 1e-12\n",
        "    iou = tp / (union + eps)\n",
        "    # If a class has zero gt pixels, set IoU to nan\n",
        "    iou[pos_gt == 0] = np.nan\n",
        "\n",
        "    mean_iou = np.nanmean(iou)\n",
        "    pixel_acc = tp.sum() / (conf.sum() + eps)\n",
        "\n",
        "    per_class = {f\"class_{c}\": float(iou[c]) if not np.isnan(iou[c]) else None\n",
        "                 for c in range(len(iou))}\n",
        "    metrics = {\"per_class_iou\": per_class, \"mean_iou\": float(mean_iou), \"pixel_acc\": float(pixel_acc)}\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "axgw7UtQobTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------\n",
        "# Training / Validation loops\n",
        "# -------------------------\n",
        "def train_one_epoch(model, dataloader, optimizer, device, criterion, num_classes, epoch, log_every=20):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    conf = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for i, (images, masks) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)['out']  # (N, C, H, W)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)  # (N,H,W)\n",
        "        conf += fast_confusion_matrix(preds, masks, num_classes)\n",
        "\n",
        "        if (i + 1) % log_every == 0:\n",
        "            print(f\"  [Epoch {epoch}] Iter {i+1}/{len(dataloader)}  loss={loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / (len(dataloader.dataset))\n",
        "    metrics = compute_metrics_from_confusion(conf, ignore_index=None)\n",
        "    metrics['loss'] = epoch_loss\n",
        "    return metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, dataloader, device, criterion, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    conf = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs, masks)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        conf += fast_confusion_matrix(preds, masks, num_classes)\n",
        "\n",
        "    epoch_loss = running_loss / (len(dataloader.dataset))\n",
        "    metrics = compute_metrics_from_confusion(conf, ignore_index=None)\n",
        "    metrics['loss'] = epoch_loss\n",
        "    return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "w7GSxJL-oewM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGx7Lq4SRoAG"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Arguments\n",
        "# -------------------------\n",
        "\n",
        "train_dir='/content/drive/MyDrive/endovis256/train/'\n",
        "val_dir='/content/drive/MyDrive/endovis256/test/'\n",
        "out_dir='/content/drive/MyDrive/endovis256/checkpoints/'\n",
        "epochs=20\n",
        "batch_size=32\n",
        "img_size=256\n",
        "lr=1e-4\n",
        "num_workers=32\n",
        "num_classes=10\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Arguments\n",
        "# -------------------------\n",
        "\n",
        "def main():\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Preparing datasets...\")\n",
        "    train_dataset = VideoSegmentationDataset(train_dir, img_size=(img_size, img_size), augment=True)\n",
        "    val_dataset = VideoSegmentationDataset(val_dir, img_size=(img_size, img_size), augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    print(\"Creating model...\")\n",
        "    # Use torchvision's DeepLabV3 with ResNet-50 backbone\n",
        "    model = seg_models.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
        "    # for using the pretrained model, uncomment the lines below instead\n",
        "    # model = seg_models.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
        "    # ckpt = torch.load('/content/drive/MyDrive/endovis256/checkpoints/best_model.pth', map_location=device)\n",
        "    # model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "    best_miou = -1.0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
        "        train_metrics = train_one_epoch(model, train_loader, optimizer, device, criterion, num_classes, epoch)\n",
        "        print(f\"Train loss {train_metrics['loss']:.4f}  pix_acc {train_metrics['pixel_acc']:.4f}  mean_iou {train_metrics['mean_iou']:.4f}\")\n",
        "\n",
        "        val_metrics = validate(model, val_loader, device, criterion, num_classes)\n",
        "        print(f\"Val   loss {val_metrics['loss']:.4f}  pix_acc {val_metrics['pixel_acc']:.4f}  mean_iou {val_metrics['mean_iou']:.4f}\")\n",
        "        for k, v in val_metrics['per_class_iou'].items():\n",
        "            print(f\"  {k}: {v}\")\n",
        "\n",
        "        scheduler.step(val_metrics['mean_iou'])\n",
        "\n",
        "        # # Save checkpoint\n",
        "        # ckpt_path = os.path.join(out_dir, f\"epoch_{epoch:03d}.pth\")\n",
        "        # torch.save({\n",
        "        #     'epoch': epoch,\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #     'miou': val_metrics['mean_iou']\n",
        "        # }, ckpt_path)\n",
        "\n",
        "        if val_metrics['mean_iou'] > best_miou:\n",
        "            best_miou = val_metrics['mean_iou']\n",
        "            best_path = os.path.join(out_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'miou': val_metrics['mean_iou']\n",
        "            }, best_path)\n",
        "            print(f\"  -> New best model saved to {best_path} (mIoU={best_miou:.4f})\")\n",
        "\n",
        "    print(\"Training finished. Best val mIoU:\", best_miou)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4tHTd1jloots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oul1GckHqH2Z",
        "outputId": "022bbb7f-0d6d-4346-af06-42807b9e8f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 132MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Epoch 1/20 ===\n",
            "  [Epoch 1] Iter 20/408  loss=0.0664\n",
            "  [Epoch 1] Iter 40/408  loss=0.0564\n",
            "  [Epoch 1] Iter 60/408  loss=0.0725\n",
            "  [Epoch 1] Iter 80/408  loss=0.0806\n",
            "  [Epoch 1] Iter 100/408  loss=0.0648\n",
            "  [Epoch 1] Iter 120/408  loss=0.0726\n",
            "  [Epoch 1] Iter 140/408  loss=0.0603\n",
            "  [Epoch 1] Iter 160/408  loss=0.0661\n",
            "  [Epoch 1] Iter 180/408  loss=0.0674\n",
            "  [Epoch 1] Iter 200/408  loss=0.0733\n",
            "  [Epoch 1] Iter 220/408  loss=0.0627\n",
            "  [Epoch 1] Iter 240/408  loss=0.0617\n",
            "  [Epoch 1] Iter 260/408  loss=0.0635\n",
            "  [Epoch 1] Iter 280/408  loss=0.0713\n",
            "  [Epoch 1] Iter 300/408  loss=0.0717\n",
            "  [Epoch 1] Iter 320/408  loss=0.0627\n",
            "  [Epoch 1] Iter 340/408  loss=0.0747\n",
            "  [Epoch 1] Iter 360/408  loss=0.0672\n",
            "  [Epoch 1] Iter 380/408  loss=0.0687\n",
            "  [Epoch 1] Iter 400/408  loss=0.0608\n",
            "Train loss 0.0685  pix_acc 0.9733  mean_iou 0.7372\n",
            "Val   loss 0.1251  pix_acc 0.9600  mean_iou 0.6427\n",
            "  class_0: 0.9606684891676613\n",
            "  class_1: 0.7009889498790224\n",
            "  class_2: 0.7525047174510284\n",
            "  class_3: 0.9149820994522903\n",
            "  class_4: 0.4974997947535979\n",
            "  class_5: 0.24754709881171416\n",
            "  class_6: 0.7283035089084261\n",
            "  class_7: 0.7053628542473748\n",
            "  class_8: 0.11745709617038219\n",
            "  class_9: 0.8017682514469774\n",
            "  -> New best model saved to /content/drive/MyDrive/endovis256/checkpoints/best_model.pth (mIoU=0.6427)\n",
            "\n",
            "=== Epoch 2/20 ===\n",
            "  [Epoch 2] Iter 20/408  loss=0.0671\n",
            "  [Epoch 2] Iter 40/408  loss=0.0602\n",
            "  [Epoch 2] Iter 60/408  loss=0.0672\n",
            "  [Epoch 2] Iter 80/408  loss=0.0541\n",
            "  [Epoch 2] Iter 100/408  loss=0.0639\n",
            "  [Epoch 2] Iter 120/408  loss=0.0676\n",
            "  [Epoch 2] Iter 140/408  loss=0.0631\n",
            "  [Epoch 2] Iter 160/408  loss=0.0599\n",
            "  [Epoch 2] Iter 180/408  loss=0.0580\n",
            "  [Epoch 2] Iter 200/408  loss=0.0702\n",
            "  [Epoch 2] Iter 220/408  loss=0.0631\n",
            "  [Epoch 2] Iter 240/408  loss=0.0687\n",
            "  [Epoch 2] Iter 260/408  loss=0.0711\n",
            "  [Epoch 2] Iter 280/408  loss=0.0610\n",
            "  [Epoch 2] Iter 300/408  loss=0.0656\n",
            "  [Epoch 2] Iter 320/408  loss=0.0627\n",
            "  [Epoch 2] Iter 340/408  loss=0.0653\n",
            "  [Epoch 2] Iter 360/408  loss=0.0633\n",
            "  [Epoch 2] Iter 380/408  loss=0.0708\n",
            "  [Epoch 2] Iter 400/408  loss=0.0692\n",
            "Train loss 0.0646  pix_acc 0.9744  mean_iou 0.7483\n",
            "Val   loss 0.1308  pix_acc 0.9597  mean_iou 0.6568\n",
            "  class_0: 0.9603652933721772\n",
            "  class_1: 0.6979224980647362\n",
            "  class_2: 0.7452570701354331\n",
            "  class_3: 0.9128449928637287\n",
            "  class_4: 0.5137013274699309\n",
            "  class_5: 0.27095375184899007\n",
            "  class_6: 0.7242511328276877\n",
            "  class_7: 0.7898455046322291\n",
            "  class_8: 0.16283957739106283\n",
            "  class_9: 0.7900611448456586\n",
            "  -> New best model saved to /content/drive/MyDrive/endovis256/checkpoints/best_model.pth (mIoU=0.6568)\n",
            "\n",
            "=== Epoch 3/20 ===\n",
            "  [Epoch 3] Iter 20/408  loss=0.0578\n",
            "  [Epoch 3] Iter 40/408  loss=0.0583\n",
            "  [Epoch 3] Iter 60/408  loss=0.0653\n",
            "  [Epoch 3] Iter 80/408  loss=0.0687\n",
            "  [Epoch 3] Iter 100/408  loss=0.0598\n",
            "  [Epoch 3] Iter 120/408  loss=0.0662\n",
            "  [Epoch 3] Iter 140/408  loss=0.0628\n",
            "  [Epoch 3] Iter 160/408  loss=0.0600\n",
            "  [Epoch 3] Iter 180/408  loss=0.0537\n",
            "  [Epoch 3] Iter 200/408  loss=0.0588\n",
            "  [Epoch 3] Iter 220/408  loss=0.0626\n",
            "  [Epoch 3] Iter 240/408  loss=0.0625\n",
            "  [Epoch 3] Iter 260/408  loss=0.0602\n",
            "  [Epoch 3] Iter 280/408  loss=0.0566\n",
            "  [Epoch 3] Iter 300/408  loss=0.0635\n",
            "  [Epoch 3] Iter 320/408  loss=0.0629\n",
            "  [Epoch 3] Iter 340/408  loss=0.0603\n",
            "  [Epoch 3] Iter 360/408  loss=0.0554\n",
            "  [Epoch 3] Iter 380/408  loss=0.0642\n",
            "  [Epoch 3] Iter 400/408  loss=0.0602\n",
            "Train loss 0.0621  pix_acc 0.9752  mean_iou 0.7545\n",
            "Val   loss 0.1269  pix_acc 0.9601  mean_iou 0.6372\n",
            "  class_0: 0.9608257637331411\n",
            "  class_1: 0.7150647364656235\n",
            "  class_2: 0.7583836424837326\n",
            "  class_3: 0.9122189854390484\n",
            "  class_4: 0.48884191828919493\n",
            "  class_5: 0.24701416667072681\n",
            "  class_6: 0.7155783159508239\n",
            "  class_7: 0.6274306300955061\n",
            "  class_8: 0.14401749282307125\n",
            "  class_9: 0.8025938400827651\n",
            "\n",
            "=== Epoch 4/20 ===\n",
            "  [Epoch 4] Iter 20/408  loss=0.0601\n",
            "  [Epoch 4] Iter 40/408  loss=0.0575\n",
            "  [Epoch 4] Iter 60/408  loss=0.0566\n",
            "  [Epoch 4] Iter 80/408  loss=0.0661\n",
            "  [Epoch 4] Iter 100/408  loss=0.0595\n",
            "  [Epoch 4] Iter 120/408  loss=0.0613\n",
            "  [Epoch 4] Iter 140/408  loss=0.0577\n",
            "  [Epoch 4] Iter 160/408  loss=0.0534\n",
            "  [Epoch 4] Iter 180/408  loss=0.0568\n",
            "  [Epoch 4] Iter 200/408  loss=0.0659\n",
            "  [Epoch 4] Iter 220/408  loss=0.0491\n",
            "  [Epoch 4] Iter 240/408  loss=0.0523\n",
            "  [Epoch 4] Iter 260/408  loss=0.0561\n",
            "  [Epoch 4] Iter 280/408  loss=0.0560\n",
            "  [Epoch 4] Iter 300/408  loss=0.0597\n",
            "  [Epoch 4] Iter 320/408  loss=0.0651\n",
            "  [Epoch 4] Iter 340/408  loss=0.0510\n",
            "  [Epoch 4] Iter 360/408  loss=0.0571\n",
            "  [Epoch 4] Iter 380/408  loss=0.0558\n",
            "  [Epoch 4] Iter 400/408  loss=0.0595\n",
            "Train loss 0.0592  pix_acc 0.9760  mean_iou 0.7628\n",
            "Val   loss 0.1285  pix_acc 0.9604  mean_iou 0.6568\n",
            "  class_0: 0.9613756632282157\n",
            "  class_1: 0.7134157746090225\n",
            "  class_2: 0.7538785261232162\n",
            "  class_3: 0.9109828344451812\n",
            "  class_4: 0.5071128674698795\n",
            "  class_5: 0.265733979006132\n",
            "  class_6: 0.7345625123674501\n",
            "  class_7: 0.7358664768611787\n",
            "  class_8: 0.181788334903734\n",
            "  class_9: 0.8033738149992105\n",
            "  -> New best model saved to /content/drive/MyDrive/endovis256/checkpoints/best_model.pth (mIoU=0.6568)\n",
            "\n",
            "=== Epoch 5/20 ===\n",
            "  [Epoch 5] Iter 20/408  loss=0.0602\n",
            "  [Epoch 5] Iter 40/408  loss=0.0487\n",
            "  [Epoch 5] Iter 60/408  loss=0.0560\n",
            "  [Epoch 5] Iter 80/408  loss=0.0529\n",
            "  [Epoch 5] Iter 100/408  loss=0.0548\n",
            "  [Epoch 5] Iter 120/408  loss=0.0546\n",
            "  [Epoch 5] Iter 140/408  loss=0.0614\n",
            "  [Epoch 5] Iter 160/408  loss=0.0677\n",
            "  [Epoch 5] Iter 180/408  loss=0.0556\n",
            "  [Epoch 5] Iter 200/408  loss=0.0618\n",
            "  [Epoch 5] Iter 220/408  loss=0.0596\n",
            "  [Epoch 5] Iter 240/408  loss=0.0543\n",
            "  [Epoch 5] Iter 260/408  loss=0.0574\n",
            "  [Epoch 5] Iter 280/408  loss=0.0518\n",
            "  [Epoch 5] Iter 300/408  loss=0.0567\n",
            "  [Epoch 5] Iter 320/408  loss=0.0635\n",
            "  [Epoch 5] Iter 340/408  loss=0.0563\n",
            "  [Epoch 5] Iter 360/408  loss=0.0513\n",
            "  [Epoch 5] Iter 380/408  loss=0.0544\n",
            "  [Epoch 5] Iter 400/408  loss=0.0628\n",
            "Train loss 0.0569  pix_acc 0.9767  mean_iou 0.7690\n",
            "Val   loss 0.1325  pix_acc 0.9608  mean_iou 0.6529\n",
            "  class_0: 0.9611221676427374\n",
            "  class_1: 0.7170079769813895\n",
            "  class_2: 0.7543845540282984\n",
            "  class_3: 0.9138934694433924\n",
            "  class_4: 0.486563366080898\n",
            "  class_5: 0.24470583786245662\n",
            "  class_6: 0.7545550686008863\n",
            "  class_7: 0.7195201753190803\n",
            "  class_8: 0.1719366844873767\n",
            "  class_9: 0.8052171974419228\n",
            "\n",
            "=== Epoch 6/20 ===\n",
            "  [Epoch 6] Iter 20/408  loss=0.0568\n",
            "  [Epoch 6] Iter 40/408  loss=0.0575\n",
            "  [Epoch 6] Iter 60/408  loss=0.0475\n",
            "  [Epoch 6] Iter 80/408  loss=0.0534\n",
            "  [Epoch 6] Iter 100/408  loss=0.0559\n",
            "  [Epoch 6] Iter 120/408  loss=0.0603\n",
            "  [Epoch 6] Iter 140/408  loss=0.0533\n",
            "  [Epoch 6] Iter 160/408  loss=0.0573\n",
            "  [Epoch 6] Iter 180/408  loss=0.0572\n",
            "  [Epoch 6] Iter 200/408  loss=0.0586\n",
            "  [Epoch 6] Iter 220/408  loss=0.0553\n",
            "  [Epoch 6] Iter 240/408  loss=0.0510\n",
            "  [Epoch 6] Iter 260/408  loss=0.0566\n",
            "  [Epoch 6] Iter 280/408  loss=0.0528\n",
            "  [Epoch 6] Iter 300/408  loss=0.0526\n",
            "  [Epoch 6] Iter 320/408  loss=0.0563\n",
            "  [Epoch 6] Iter 340/408  loss=0.0532\n",
            "  [Epoch 6] Iter 360/408  loss=0.0570\n",
            "  [Epoch 6] Iter 380/408  loss=0.0551\n",
            "  [Epoch 6] Iter 400/408  loss=0.0544\n",
            "Train loss 0.0554  pix_acc 0.9772  mean_iou 0.7728\n",
            "Val   loss 0.1371  pix_acc 0.9596  mean_iou 0.6553\n",
            "  class_0: 0.9606003376599977\n",
            "  class_1: 0.7122852735426417\n",
            "  class_2: 0.7434870452612498\n",
            "  class_3: 0.9093215235864037\n",
            "  class_4: 0.5141057196900747\n",
            "  class_5: 0.27398369709788106\n",
            "  class_6: 0.7275481826161937\n",
            "  class_7: 0.7448568066840715\n",
            "  class_8: 0.17450842226253024\n",
            "  class_9: 0.7919471151722274\n",
            "\n",
            "=== Epoch 7/20 ===\n",
            "  [Epoch 7] Iter 20/408  loss=0.0529\n",
            "  [Epoch 7] Iter 40/408  loss=0.0533\n",
            "  [Epoch 7] Iter 60/408  loss=0.0523\n",
            "  [Epoch 7] Iter 80/408  loss=0.0537\n",
            "  [Epoch 7] Iter 100/408  loss=0.0463\n",
            "  [Epoch 7] Iter 120/408  loss=0.0514\n",
            "  [Epoch 7] Iter 140/408  loss=0.0483\n",
            "  [Epoch 7] Iter 160/408  loss=0.0519\n",
            "  [Epoch 7] Iter 180/408  loss=0.0551\n",
            "  [Epoch 7] Iter 200/408  loss=0.0484\n",
            "  [Epoch 7] Iter 220/408  loss=0.0585\n",
            "  [Epoch 7] Iter 240/408  loss=0.0528\n",
            "  [Epoch 7] Iter 260/408  loss=0.0504\n",
            "  [Epoch 7] Iter 280/408  loss=0.0471\n",
            "  [Epoch 7] Iter 300/408  loss=0.0496\n",
            "  [Epoch 7] Iter 320/408  loss=0.0517\n",
            "  [Epoch 7] Iter 340/408  loss=0.0523\n",
            "  [Epoch 7] Iter 360/408  loss=0.0502\n",
            "  [Epoch 7] Iter 380/408  loss=0.0463\n",
            "  [Epoch 7] Iter 400/408  loss=0.0480\n",
            "Train loss 0.0516  pix_acc 0.9785  mean_iou 0.7825\n",
            "Val   loss 0.1395  pix_acc 0.9612  mean_iou 0.6585\n",
            "  class_0: 0.9614741958192327\n",
            "  class_1: 0.7182354251171517\n",
            "  class_2: 0.7588031230713206\n",
            "  class_3: 0.9155630851476293\n",
            "  class_4: 0.5239463182952494\n",
            "  class_5: 0.2702384162846213\n",
            "  class_6: 0.7391922867915761\n",
            "  class_7: 0.7524234519295026\n",
            "  class_8: 0.136955102872612\n",
            "  class_9: 0.8079343400176149\n",
            "  -> New best model saved to /content/drive/MyDrive/endovis256/checkpoints/best_model.pth (mIoU=0.6585)\n",
            "\n",
            "=== Epoch 8/20 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-451043146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1693193419.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Epoch {epoch}/{epochs} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train loss {train_metrics['loss']:.4f}  pix_acc {train_metrics['pixel_acc']:.4f}  mean_iou {train_metrics['mean_iou']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3601911261.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device, criterion, num_classes, epoch, log_every)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N,H,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}